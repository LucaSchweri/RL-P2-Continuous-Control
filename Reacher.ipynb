{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Project 1: Navigation\n",
    "\n",
    "The following code initialized the Unity environment. If a new agent is trained please restart the kernel."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unityagents:\n",
      "'Academy' started successfully!\n",
      "Unity Academy name: Academy\n",
      "        Number of Brains: 1\n",
      "        Number of External Brains : 1\n",
      "        Lesson number : 0\n",
      "        Reset Parameters :\n",
      "\t\tgoal_speed -> 1.0\n",
      "\t\tgoal_size -> 5.0\n",
      "Unity brain name: ReacherBrain\n",
      "        Number of Visual Observations (per agent): 0\n",
      "        Vector Observation space type: continuous\n",
      "        Vector Observation space size (per agent): 33\n",
      "        Number of stacked Vector Observation: 1\n",
      "        Vector Action space type: continuous\n",
      "        Vector Action space size (per agent): 4\n",
      "        Vector Action descriptions: , , , \n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from unityagents import UnityEnvironment\n",
    "\n",
    "# create environment\n",
    "\n",
    "env = UnityEnvironment(file_name='Reacher_20_Windows_x86_64/Reacher.exe', seed=0) # chnage path of environment if needed\n",
    "brain_name = env.brain_names[0]\n",
    "brain = env.brains[brain_name]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Trainig\n",
    "\n",
    "The following code can be used to train the agent. You can change the configuration of the agent as well as which agent and network should be used in the [config.json](config.json) file. This is a list of the configurations:\n",
    "- **name**: The name of the approach used for saving and loading the agent and its statistics\n",
    "- **agent**: The name of the agent to use\n",
    "- **num_episode**: The number of episodes used during training\n",
    "- **agents**: The configurations for the different agent available"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 10\tAverage Loss: -0.37922\tAverage Score: 0.91280\n",
      "Episode 20\tAverage Loss: -0.42165\tAverage Score: 0.77952\n",
      "Episode 30\tAverage Loss: -0.44924\tAverage Score: 0.57840\n",
      "Episode 40\tAverage Loss: -0.46576\tAverage Score: 0.78965\n",
      "Episode 50\tAverage Loss: -0.47739\tAverage Score: 1.23719\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "\u001B[1;32m<ipython-input-2-b8aebc17e2c9>\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m     53\u001B[0m         \u001B[0mreward\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0menv_info\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mrewards\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     54\u001B[0m         \u001B[0mdone\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0menv_info\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mlocal_done\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 55\u001B[1;33m         \u001B[0mloss\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0magent\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mstep\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mstate\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0maction\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mreward\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mnext_state\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mdone\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     56\u001B[0m         \u001B[1;32mif\u001B[0m \u001B[0mloss\u001B[0m \u001B[1;32mis\u001B[0m \u001B[1;32mnot\u001B[0m \u001B[1;32mNone\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     57\u001B[0m             \u001B[0mlosses\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mappend\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mnp\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mmean\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mloss\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mD:\\Projekte\\reinforcment_learning\\P2 Continuous Control\\agents\\ddpg_agent.py\u001B[0m in \u001B[0;36mstep\u001B[1;34m(self, state, action, reward, next_state, is_done)\u001B[0m\n\u001B[0;32m    105\u001B[0m             \u001B[1;31m# Learn\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    106\u001B[0m             \u001B[1;32mif\u001B[0m \u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mt_step\u001B[0m \u001B[1;33m+\u001B[0m \u001B[1;36m1\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;33m%\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mupdate_net_steps\u001B[0m \u001B[1;33m==\u001B[0m \u001B[1;36m0\u001B[0m \u001B[1;32mand\u001B[0m \u001B[0mlen\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mreplay_buffer\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;33m>=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mbatch_size\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 107\u001B[1;33m                 \u001B[0mloss\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mlearn\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    108\u001B[0m                 \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mlosses\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mappend\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mloss\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    109\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mD:\\Projekte\\reinforcment_learning\\P2 Continuous Control\\agents\\ddpg_agent.py\u001B[0m in \u001B[0;36mlearn\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    146\u001B[0m         \u001B[0mlosses\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;33m[\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    147\u001B[0m         \u001B[1;32mfor\u001B[0m \u001B[0m_\u001B[0m \u001B[1;32min\u001B[0m \u001B[0mrange\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mrepeated_update\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 148\u001B[1;33m             \u001B[0mstates\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mactions\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mrewards\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mnext_states\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mdones\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mcounts\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mreplay_buffer\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0msample\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    149\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    150\u001B[0m             \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mtarget_actor_net\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0meval\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mD:\\Projekte\\reinforcment_learning\\P2 Continuous Control\\agents\\ddpg_agent.py\u001B[0m in \u001B[0;36msample\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    260\u001B[0m         \u001B[0mexperiences\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;33m[\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mbuffer\u001B[0m\u001B[1;33m[\u001B[0m\u001B[0mint\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mi\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m]\u001B[0m \u001B[1;32mfor\u001B[0m \u001B[0mi\u001B[0m \u001B[1;32min\u001B[0m \u001B[0midx\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    261\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 262\u001B[1;33m         \u001B[0mstates\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mtorch\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mfrom_numpy\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mnp\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mvstack\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m[\u001B[0m\u001B[0me\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;36m0\u001B[0m\u001B[1;33m]\u001B[0m \u001B[1;32mfor\u001B[0m \u001B[0me\u001B[0m \u001B[1;32min\u001B[0m \u001B[0mexperiences\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mfloat\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mto\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mdevice\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    263\u001B[0m         \u001B[0mactions\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mtorch\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mfrom_numpy\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mnp\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mvstack\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m[\u001B[0m\u001B[0me\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;36m1\u001B[0m\u001B[1;33m]\u001B[0m \u001B[1;32mfor\u001B[0m \u001B[0me\u001B[0m \u001B[1;32min\u001B[0m \u001B[0mexperiences\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mfloat\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mto\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mdevice\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    264\u001B[0m         \u001B[0mrewards\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mtorch\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mfrom_numpy\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mnp\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mvstack\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mnp\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mexpand_dims\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m[\u001B[0m\u001B[0me\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;36m2\u001B[0m\u001B[1;33m]\u001B[0m \u001B[1;32mfor\u001B[0m \u001B[0me\u001B[0m \u001B[1;32min\u001B[0m \u001B[0mexperiences\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0maxis\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;33m-\u001B[0m\u001B[1;36m1\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mfloat\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mto\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mdevice\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m<__array_function__ internals>\u001B[0m in \u001B[0;36mvstack\u001B[1;34m(*args, **kwargs)\u001B[0m\n",
      "\u001B[1;32m~\\anaconda3\\envs\\udacity_rl\\lib\\site-packages\\numpy\\core\\shape_base.py\u001B[0m in \u001B[0;36mvstack\u001B[1;34m(tup)\u001B[0m\n\u001B[0;32m    281\u001B[0m     \u001B[1;32mif\u001B[0m \u001B[1;32mnot\u001B[0m \u001B[0misinstance\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0marrs\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mlist\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    282\u001B[0m         \u001B[0marrs\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;33m[\u001B[0m\u001B[0marrs\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 283\u001B[1;33m     \u001B[1;32mreturn\u001B[0m \u001B[0m_nx\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mconcatenate\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0marrs\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;36m0\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    284\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    285\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m<__array_function__ internals>\u001B[0m in \u001B[0;36mconcatenate\u001B[1;34m(*args, **kwargs)\u001B[0m\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "\n",
    "from importlib import reload\n",
    "import agents.agents\n",
    "reload(agents.agents)\n",
    "from agents.agents import get_agent\n",
    "import utils.visualizations\n",
    "reload(utils.visualizations)\n",
    "import utils.visualizations as vis\n",
    "\n",
    "\n",
    "# load current config\n",
    "with open(\"config.json\", \"r\") as f:\n",
    "    config = json.load(f)\n",
    "\n",
    "# create results folder\n",
    "if not os.path.exists(\"./data\"):\n",
    "    os.mkdir(\"./data\")\n",
    "if not os.path.exists(f\"./data/{config['name']}\"):\n",
    "    os.mkdir(f\"./data/{config['name']}\")\n",
    "\n",
    "# copy current config to result folder\n",
    "with open(f\"./data/{config['name']}/config.json\", \"w\") as f2:\n",
    "    json.dump(config, f2)\n",
    "\n",
    "# get env infos\n",
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "action_size = brain.vector_action_space_size\n",
    "state_size = len(env_info.vector_observations[0])\n",
    "\n",
    "# create agent\n",
    "agent = get_agent(config[\"agent\"], config[\"agents\"], state_size, action_size, 0)\n",
    "\n",
    "# trains the agent and collect statistics\n",
    "scores = []\n",
    "losses = []\n",
    "for i_episode in range(1, config[\"num_episodes\"]+1):\n",
    "    score = 0\n",
    "\n",
    "    env_info = env.reset(train_mode=True)[brain_name]\n",
    "    state = env_info.vector_observations\n",
    "\n",
    "    # one episode\n",
    "    while True:\n",
    "        # select action\n",
    "        action = agent.act(state)\n",
    "\n",
    "        # get feddback from environment\n",
    "        env_info = env.step(action)[brain_name]\n",
    "        next_state = env_info.vector_observations\n",
    "        reward = env_info.rewards\n",
    "        done = env_info.local_done\n",
    "        loss = agent.step(state, action, reward, next_state, done)\n",
    "        if loss is not None:\n",
    "            losses.append(np.mean(loss))\n",
    "\n",
    "        state = next_state\n",
    "        score += np.mean(reward)\n",
    "        if np.any(done):\n",
    "            # episode ended\n",
    "            break\n",
    "\n",
    "    #statistics\n",
    "    scores.append(score)\n",
    "    if i_episode < 100:\n",
    "        print('\\rEpisode {}\\tAverage Loss: {:.5f}\\tAverage Score: {:.5f}'.format(i_episode, np.mean(losses), np.mean(scores)), end=\"\")\n",
    "    else:\n",
    "        print('\\rEpisode {}\\tAverage Loss: {:.5f}\\tAverage Score: {:.5f}'.format(i_episode, np.mean(losses[-100:]), np.mean(scores[-100:])), end=\"\")\n",
    "    if i_episode % 10 == 0:\n",
    "        if i_episode < 100:\n",
    "            print('\\rEpisode {}\\tAverage Loss: {:.5f}\\tAverage Score: {:.5f}'.format(i_episode, np.mean(losses), np.mean(scores)))\n",
    "        else:\n",
    "            print('\\rEpisode {}\\tAverage Loss: {:.5f}\\tAverage Score: {:.5f}'.format(i_episode, np.mean(losses[-100:]), np.mean(scores[-100:])))\n",
    "\n",
    "# save agent and statistics\n",
    "agent.save(config[\"name\"])\n",
    "np.savez(f\"./data/{config['name']}/statistics.npz\", scores=scores, losses=losses)\n",
    "\n",
    "# plot statistics\n",
    "for show in [False, True]:\n",
    "    vis.plot_learning_curve([scores], [config[\"name\"]], show=show)\n",
    "    vis.plot_learning_curve2([losses], [config[\"name\"]], show=show)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Test\n",
    "\n",
    "The following code can be used to test the saved agent for one episode. You can change which agent to test in the [config.json](config.json) file."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 28.268999368138566\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "from importlib import reload\n",
    "import agents.agents\n",
    "reload(agents.agents)\n",
    "from agents.agents import get_agent\n",
    "import utils.visualizations\n",
    "reload(utils.visualizations)\n",
    "import utils.visualizations as vis\n",
    "\n",
    "# load current config\n",
    "with open(\"config.json\", \"r\") as f:\n",
    "    config = json.load(f)\n",
    "with open(f\"./data/{config['name']}/config.json\", \"r\") as f:\n",
    "    config = json.load(f)\n",
    "\n",
    "# get env infos\n",
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "action_size = brain.vector_action_space_size\n",
    "state_size = len(env_info.vector_observations[0])\n",
    "\n",
    "# create agent and load trained parameters\n",
    "agent = get_agent(config[\"agent\"], config[\"agents\"], state_size, action_size, 0)\n",
    "agent.load(config[\"name\"])\n",
    "\n",
    "# one episode test\n",
    "env_info = env.reset(train_mode=False)[brain_name]\n",
    "state = env_info.vector_observations\n",
    "score = 0\n",
    "while True:\n",
    "    action = agent.act(state, test=True)\n",
    "    env_info = env.step(action)[brain_name]\n",
    "    next_state = env_info.vector_observations\n",
    "    reward = env_info.rewards\n",
    "    done = env_info.local_done\n",
    "    score += np.mean(reward)\n",
    "    state = next_state\n",
    "    if np.any(done):\n",
    "        break\n",
    "\n",
    "# statistics\n",
    "print(\"Score: {}\".format(score))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "If you don't need the environment anymore, you can close it."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "env.close()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Comparison\n",
    "\n",
    "With the following code you can compare the statistics of different approaches. For this, specify the names of the approaches below (variable: \"names\")."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from importlib import reload\n",
    "import utils.visualizations\n",
    "reload(utils.visualizations)\n",
    "import utils.visualizations as vis\n",
    "\n",
    "# specify the names of the approaches to compare\n",
    "names = [\"gaussian_ppo\", \"gaussian_a2c\", \"ddpg\"]\n",
    "\n",
    "# load statistics\n",
    "scores = []\n",
    "avg_q_values = []\n",
    "avg_target_diffs = []\n",
    "for name in names:\n",
    "    data = dict(np.load(f\"./data/{name}/statistics.npz\"))\n",
    "    scores.append(data[\"scores\"])\n",
    "\n",
    "# plot statistics\n",
    "vis.plot_learning_curve(scores, names)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}